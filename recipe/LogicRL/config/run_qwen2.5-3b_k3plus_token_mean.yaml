hydra:
  searchpath:
    - file://verl/trainer/config

defaults:
  - ppo_trainer
  - _self_

# Data configuration
data:
  train_files: /root/verl/recipe/LogicRL/data/processed_data/3ppl/train.parquet
  val_files: /root/verl/recipe/LogicRL/data/processed_data/3ppl/test.parquet
  train_batch_size: 16
  max_prompt_length: 400
  max_response_length: 4096

# Actor, rollout and reference model configuration
actor_rollout_ref:
  model:
    path: /root/models/Qwen2.5-3B-Instruct
    enable_gradient_checkpointing: True
    use_fused_kernels: True
    fused_kernel_options:
      impl_backend: triton

  actor:
    ppo_mini_batch_size: 4  
    ppo_micro_batch_size_per_gpu: 1  
    loss_agg_mode: token-mean 
    use_kl_loss: True
    kl_loss_coef: 0.001
    kl_loss_type: k3+
    clip_ratio_low: 0.2
    clip_ratio_high: 0.2
    optim:
      lr: 4e-7
    fsdp_config:
      model_dtype: bfloat16

  
  rollout:
    name: vllm
    dtype: bfloat16
    log_prob_micro_batch_size_per_gpu: 120  
    tensor_model_parallel_size: 2  # Use 2 GPUs for tensor parallel in vLLM rollout
    gpu_memory_utilization: 0.7  # Increased to better utilize GPU memory
    n: 8  # Generate 8 responses per prompt for GRPO (group sampling)
  
  ref:
    log_prob_micro_batch_size_per_gpu: 120
    fsdp_config:
      param_offload: True

algorithm:
  adv_estimator: grpo
  use_kl_in_reward: False

custom_reward_function:
  path: /root/verl/recipe/LogicRL/reward_function.py
  name: compute_score
  
trainer:
  total_epochs: 5
  project_name: Logic-RL
  experiment_name: qwen2.5-3b_k3plus_token_mean
  logger: ['console', 'swanlab']
  log_val_generations: 10
  nnodes: 1
  n_gpus_per_node: 4
  save_freq: 50
  val_before_train: True
  test_freq: 20
  critic_warmup: 0
